import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pickle # C·∫ßn thi·∫øt ƒë·ªÉ save/load model n·∫øu kh√¥ng d√πng @st.cache_resource

# Th∆∞ vi·ªán Preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

# Th∆∞ vi·ªán M√¥ h√¨nh Clustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

# ========================================================
# 0. STREAMLIT CONFIGURATION
# ========================================================
st.set_page_config(page_title="Ph√¢n kh√∫c Kh√°ch h√†ng", page_icon="üë•", layout="wide")
st.title("üë• M√¥ h√¨nh Ph√¢n kh√∫c Kh√°ch h√†ng (K-Means)")
st.caption("D·ª±a tr√™n c√°c ƒë·∫∑c tr∆∞ng RFM, H√†nh vi giao d·ªãch v√† H·ªì s∆° c√° nh√¢n.")

# C√†i ƒë·∫∑t hi·ªÉn th·ªã (ch·ªâ √°p d·ª•ng cho code)
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', '{:.2f}'.format)

# ========================================================
# 1. T·∫¢I V√Ä K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG (FEATURE ENGINEERING)
# ========================================================

@st.cache_data
def load_and_engineer_features(data_dir="data"):
    """
    T·∫£i d·ªØ li·ªáu, x√¢y d·ª±ng c√°c ƒë·∫∑c tr∆∞ng RFM v√† h√†nh vi.
    S·ª≠ d·ª•ng @st.cache_data ƒë·ªÉ caching d·ªØ li·ªáu ƒë·∫ßu ra (df_analysis).
    """
    st.info(f"ƒêang t·∫£i d·ªØ li·ªáu v√† x√¢y d·ª±ng ƒë·∫∑c tr∆∞ng t·ª´ th∆∞ m·ª•c '{data_dir}'...")
    try:
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
        df_trans = pd.read_csv(f'{data_dir}/transaction_data.csv')
        df_cust = pd.read_csv(f'{data_dir}/customer_profile.csv')
        
        # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu ng√†y th√°ng
        df_trans['Date_Time'] = pd.to_datetime(df_trans['Date_Time'])
        
    except FileNotFoundError:
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp CSV. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c '{data_dir}' tr√™n GitHub/Local.")
        st.stop()

    # 1.1. X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng RFM
    snapshot_date = df_trans['Date_Time'].max() + pd.Timedelta(days=1)
    
    rfm_df = df_trans.groupby('Customer_ID').agg(
        Recency=('Date_Time', lambda x: (snapshot_date - x.max()).days),
        Frequency=('Transaction_ID', 'nunique'),
        Monetary=('Total_Paid', 'sum')
    ).reset_index()
    
    # 1.2. X√¢y d·ª±ng th√™m c√°c ƒë·∫∑c tr∆∞ng h√†nh vi (Gi·ªØ nguy√™n logic c·ªßa b·∫°n)
    df_trans['Is_Discount'] = (df_trans['Discount_Amount'] > 0).astype(int)
    df_trans['Is_Weekend'] = df_trans['Date_Time'].dt.dayofweek >= 5
    
    # Suy Category t·ª´ Product_ID (Gi·ªØ nguy√™n h√†m c·ªßa b·∫°n)
    def map_category(pid):
        if isinstance(pid, str):
            if pid.startswith('CF'):
                return 'Coffee'
            elif pid.startswith('TE'):
                return 'Tea'
            elif pid.startswith('FR'):
                return 'Freeze'
        return 'Other'
    
    df_trans['Category'] = df_trans['Product_ID'].apply(map_category)
    
    cust_txn_features = df_trans.groupby('Customer_ID').agg(
        Discount_Usage=('Is_Discount', 'mean'), 
        Weekend_Visit_Rate=('Is_Weekend', 'mean'),
        Coffee_Share=('Category', lambda x: (x == 'Coffee').mean()),
        Tea_Share=('Category', lambda x: (x == 'Tea').mean()),
        Freeze_Share=('Category', lambda x: (x == 'Freeze').mean())
    ).reset_index()
    
    def preferred_category(row):
        shares = {'Coffee': row['Coffee_Share'], 'Tea': row['Tea_Share'], 'Freeze': row['Freeze_Share']}
        if max(shares.values()) == 0:
            return 'Unknown'
        return max(shares, key=shares.get)
    
    cust_txn_features['Preferred_Category'] = cust_txn_features.apply(preferred_category, axis=1)
    
    # 1.3. K·∫øt h·ª£p v√† X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng H·ªì s∆° (Profile)
    df_analysis = pd.merge(df_cust, rfm_df, on='Customer_ID', how='left')
    df_analysis = pd.merge(df_analysis, cust_txn_features, on='Customer_ID', how='left')
    
    # X·ª≠ l√Ω thi·∫øu (ƒëi·ªÅn 0 ho·∫∑c 999 cho kh√°ch h√†ng kh√¥ng c√≥ giao d·ªãch)
    df_analysis['Recency'] = df_analysis['Recency'].fillna(999) 
    df_analysis[['Frequency', 'Monetary']] = df_analysis[['Frequency', 'Monetary']].fillna(0)
    behaviour_cols = ['Discount_Usage', 'Weekend_Visit_Rate', 'Coffee_Share', 'Tea_Share', 'Freeze_Share']
    df_analysis[behaviour_cols] = df_analysis[behaviour_cols].fillna(0)
    df_analysis['Preferred_Category'] = df_analysis['Preferred_Category'].fillna('Unknown')
    
    # T·∫°o ƒë·∫∑c tr∆∞ng 'Age'
    current_year = snapshot_date.year
    df_analysis['Age'] = current_year - df_analysis['YoB']
    
    # X·ª≠ l√Ω ngo·∫°i l·ªá (Capping ·ªü Ph√¢n v·ªã 99%)
    for col in ['Recency','Frequency','Monetary']:
        cap_value = df_analysis[col].quantile(0.99)
        df_analysis[col] = df_analysis[col].clip(upper=cap_value)
    
    # 1.4. Ch·ªçn ƒë·∫∑c tr∆∞ng cu·ªëi c√πng
    features_to_cluster = [
        'Age', 'Recency', 'Frequency', 'Monetary',
        'Income level', 'Membership_Tier',
        'Occupation', 'Gender'
    ]
    df_model_input = df_analysis[features_to_cluster].copy()
    
    st.success(f"Ho√†n t·∫•t B∆Ø·ªöC 1. D·ªØ li·ªáu ƒë·∫ßu v√†o c√≥ {df_model_input.shape[0]} kh√°ch h√†ng v√† {df_model_input.shape[1]} ƒë·∫∑c tr∆∞ng.")
    return df_analysis, df_model_input

df_analysis, df_model_input = load_and_engineer_features()


# ========================================================
# 2. TI·ªÄN X·ª¨ L√ù (PREPROCESSING PIPELINE)
# ========================================================

@st.cache_resource
def build_and_fit_preprocessor(df_input):
    """
    X√¢y d·ª±ng v√† hu·∫•n luy·ªán Pipeline ti·ªÅn x·ª≠ l√Ω.
    S·ª≠ d·ª•ng @st.cache_resource v√¨ Preprocessor l√† m·ªôt Model/ƒê·ªëi t∆∞·ª£ng n·∫∑ng.
    """
    st.info("ƒêang x√¢y d·ª±ng v√† hu·∫•n luy·ªán Pipeline M√£ h√≥a & Chu·∫©n h√≥a...")
    
    # ƒê·ªãnh nghƒ©a c√°c nh√≥m c·ªôt (Gi·ªØ nguy√™n ƒë·ªãnh nghƒ©a c·ªßa b·∫°n)
    numerical_features = ['Age', 'Recency', 'Frequency', 'Monetary']
    income_levels = ['< 2M', '2-5M', '5-10M', '10-20M', '20-50M', '> 50M']
    membership_tiers = ['Standard', 'Silver', 'Gold', 'Diamond']
    ordinal_features = ['Income level', 'Membership_Tier']
    nominal_features = ['Occupation', 'Gender']
    
    # X√¢y d·ª±ng c√°c pipeline con (Gi·ªØ nguy√™n logic c·ªßa b·∫°n)
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    ordinal_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OrdinalEncoder(
            categories=[income_levels, membership_tiers],
            handle_unknown='use_encoded_value',
            unknown_value=-1
        ))
    ])
    
    nominal_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
    
    # K·∫øt h·ª£p c√°c pipeline con
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numerical_features),
            ('ord', ordinal_transformer, ordinal_features),
            ('nom', nominal_transformer, nominal_features)
        ],
        remainder='passthrough'
    )
    
    # √Åp d·ª•ng preprocessor
    X_scaled = preprocessor.fit_transform(df_input)
    
    try:
        feature_names = preprocessor.get_feature_names_out()
        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)
    except Exception:
        X_scaled_df = pd.DataFrame(X_scaled)

    st.success(f"Pipeline ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t. D·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a c√≥ {X_scaled_df.shape[1]} ƒë·∫∑c tr∆∞ng.")
    return preprocessor, X_scaled_df

preprocessor, X_scaled_df = build_and_fit_preprocessor(df_model_input)
X_scaled = X_scaled_df.values # L·∫•y l·∫°i m·∫£ng numpy ƒë·ªÉ t√≠nh to√°n

# ========================================================
# 3. X√ÅC ƒê·ªäNH S·ªê C·ª§M T·ªêI ∆ØU (K)
# ========================================================

st.header("üéØ B∆∞·ªõc 3: X√°c ƒë·ªãnh S·ªë c·ª•m T·ªëi ∆∞u (K)")
st.caption("D√πng ph∆∞∆°ng ph√°p Elbow v√† Silhouette Score ƒë·ªÉ t√¨m K ph√π h·ª£p.")

@st.cache_data
def calculate_optimal_k(X_data, k_range=range(2, 9)):
    """
    T√≠nh to√°n WCSS (Inertia) v√† Silhouette Score cho c√°c K kh√°c nhau.
    """
    inertia_values = []
    silhouette_scores = []
    
    status_text = st.empty()
    for i, k in enumerate(k_range):
        status_text.text(f"ƒêang ch·∫°y K-Means v·ªõi K={k} ({i+1}/{len(k_range)})...")
        kmeans_test = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
        kmeans_test.fit(X_data)
        
        inertia_values.append(kmeans_test.inertia_)
        score = silhouette_score(X_data, kmeans_test.labels_)
        silhouette_scores.append(score)
    status_text.success("Ho√†n t·∫•t t√≠nh to√°n WCSS (Inertia) v√† Silhouette.")
    
    return list(k_range), inertia_values, silhouette_scores

K_range, inertia_values, silhouette_scores = calculate_optimal_k(X_scaled)

# 3.1. V·∫Ω bi·ªÉu ƒë·ªì Elbow v√† Silhouette (Gi·ªØ nguy√™n logic v·∫Ω c·ªßa b·∫°n)
col_elbow, col_silhouette = st.columns(2)

with col_elbow:
    st.subheader("3.1. Ph∆∞∆°ng ph√°p Elbow")
    fig, ax = plt.subplots()
    ax.plot(K_range, inertia_values, 'bo-')
    ax.set_xlabel('S·ªë c·ª•m (K)')
    ax.set_ylabel('WCSS (Inertia)')
    ax.set_title('Ph∆∞∆°ng ph√°p Elbow (Elbow Method)')
    ax.grid(True)
    st.pyplot(fig)

with col_silhouette:
    st.subheader("3.2. Ch·ªâ s·ªë Silhouette")
    fig, ax = plt.subplots()
    ax.plot(K_range, silhouette_scores, 'rs-')
    ax.set_xlabel('S·ªë c·ª•m (K)')
    ax.set_ylabel('Silhouette Score')
    ax.set_title('Ch·ªâ s·ªë Silhouette')
    ax.grid(True)
    st.pyplot(fig)


# ========================================================
# 4. HU·∫§N LUY·ªÜN V√Ä ƒê√ÅNH GI√Å M√î H√åNH K-MEANS
# ========================================================

st.header("üìä B∆∞·ªõc 4: Hu·∫•n luy·ªán M√¥ h√¨nh Cu·ªëi c√πng")

# Widget ƒë·ªÉ ng∆∞·ªùi d√πng ch·ªçn K (Ho·∫∑c gi·ªØ K=3)
N_CLUSTERS = st.slider("Ch·ªçn s·ªë c·ª•m cu·ªëi c√πng (N_CLUSTERS):", min_value=2, max_value=8, value=3)

@st.cache_resource
def train_final_model(X_data, n_clusters):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh K-Means cu·ªëi c√πng.
    S·ª≠ d·ª•ng @st.cache_resource ƒë·ªÉ caching model object.
    """
    model_name = "K-Means"
    model = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10)
    
    with st.spinner(f"ƒêang hu·∫•n luy·ªán m√¥ h√¨nh {model_name} v·ªõi K={n_clusters}..."):
        labels = model.fit_predict(X_data)
    
    # ƒê√°nh gi√° m√¥ h√¨nh
    silhouette = silhouette_score(X_data, labels)
    davies_bouldin = davies_bouldin_score(X_data, labels)
    
    results = {
        "Silhouette Score": silhouette,
        "Davies-Bouldin Index": davies_bouldin
    }
    
    return model, labels, results

model, labels, results = train_final_model(X_scaled, N_CLUSTERS)

st.subheader("4.1. K·∫øt qu·∫£ ƒê√°nh gi√° M√¥ h√¨nh")
results_df = pd.DataFrame(results, index=["K-Means"]).T
st.dataframe(results_df)

st.success(f"M√¥ h√¨nh K-Means v·ªõi K={N_CLUSTERS} ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán th√†nh c√¥ng!")


# ========================================================
# 5. GI·∫¢M CHI·ªÄU D·ªÆ LI·ªÜU B·∫∞NG PCA & TR·ª∞C QUAN H√ìA
# ========================================================

st.header("‚ú® B∆∞·ªõc 5: Tr·ª±c quan h√≥a C·ª•m (PCA)")

@st.cache_data
def run_pca_and_format(X_data, labels, model_name):
    """
    Gi·∫£m chi·ªÅu d·ªØ li·ªáu v√† t·∫°o DataFrame cho bi·ªÉu ƒë·ªì scatter.
    """
    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(X_data)
    
    df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
    df_pca[model_name] = labels
    return df_pca

df_pca = run_pca_and_format(X_scaled, labels, "K-Means")

# V·∫Ω bi·ªÉu ƒë·ªì c·ª•m (Gi·ªØ nguy√™n logic v·∫Ω c·ªßa b·∫°n)
fig, ax = plt.subplots(figsize=(10, 8))
sns.scatterplot(x='PC1', y='PC2', hue='K-Means', data=df_pca,
                palette='viridis', legend='full', alpha=0.7, ax=ax)
ax.set_title(f'K-Means Clustering - K={N_CLUSTERS} (PCA 2D)')
ax.grid(True)
st.pyplot(fig)


# ========================================================
# 6. PH√ÇN T√çCH H·ªí S∆† KH√ÅCH H√ÄNG THEO C·ª§M
# ========================================================

st.header("üìù B∆∞·ªõc 6: Ph√¢n t√≠ch H·ªì s∆° C·ª•m (Cluster Profiling)")

def mode_or_na(x):
    """Gi·ªØ nguy√™n h√†m c·ªßa b·∫°n: L·∫•y mode ho·∫∑c tr·∫£ v·ªÅ N/A."""
    m = x.mode()
    return m.iloc[0] if not m.empty else 'N/A'

@st.cache_data(show_spinner=False)
def generate_cluster_summary(df_original, labels, n_clusters):
    """
    T·∫°o b·∫£ng t√≥m t·∫Øt ƒë·∫∑c ƒëi·ªÉm chi ti·∫øt c·ªßa t·ª´ng c·ª•m.
    """
    df_analysis_labeled = df_original.copy()
    df_analysis_labeled['Cluster'] = labels
    
    # 6.1. Cluster size
    cluster_size = df_analysis_labeled.groupby('Cluster')['Customer_ID'].count().rename('Cluster_Size')
    
    # 6.2. C√°c ƒë·∫∑c tr∆∞ng s·ªë trung b√¨nh (Gi·ªØ nguy√™n ƒë·ªãnh nghƒ©a c·ªßa b·∫°n)
    numeric_profile_cols = [
        'Age', 'Recency', 'Frequency', 'Monetary',
        'Discount_Usage', 'Weekend_Visit_Rate',
        'Coffee_Share', 'Tea_Share', 'Freeze_Share'
    ]
    cluster_numeric = df_analysis_labeled.groupby('Cluster')[numeric_profile_cols].mean()
    
    # 6.3. C√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i (Mode)
    cluster_categorical = df_analysis_labeled.groupby('Cluster').agg({
        'Income level': mode_or_na,
        'Membership_Tier': mode_or_na,
        'Occupation': mode_or_na,
        'Gender': mode_or_na,
        'Preferred_Category': mode_or_na
    })
    
    cluster_categorical = cluster_categorical.rename(columns={
        'Income level': 'Income_Level_Mode',
        'Membership_Tier':'Membership_Tier_Mode',
        'Occupation': 'Occupation_Mode',
        'Gender': 'Gender_Mode',
        'Preferred_Category':'Preferred_Category_Mode'
    })
    
    # 6.4. G·ªôp t·∫•t c·∫£ th√†nh m·ªôt b·∫£ng
    cluster_summary = pd.concat([cluster_size, cluster_numeric, cluster_categorical], axis=1).reset_index()
    
    # Chuy·ªÉn c√°c t·ª∑ l·ªá sang % (Gi·ªØ nguy√™n logic c·ªßa b·∫°n)
    ratio_cols = ['Discount_Usage', 'Weekend_Visit_Rate', 'Coffee_Share', 'Tea_Share', 'Freeze_Share']
    cluster_summary[ratio_cols] = cluster_summary[ratio_cols] * 100
    
    return cluster_summary

cluster_summary = generate_cluster_summary(df_analysis, labels, N_CLUSTERS)

st.subheader(f"6.1. B·∫£ng T√≥m t·∫Øt ƒê·∫∑c ƒëi·ªÉm C·ª•m (K={N_CLUSTERS})")
st.dataframe(cluster_summary)

st.info("üí° L∆∞u √Ω: C·ªôt **Cluster_Size** th·ªÉ hi·ªán s·ªë l∆∞·ª£ng kh√°ch h√†ng trong c·ª•m. C√°c c·ªôt **Share** v√† **Usage** ƒë∆∞·ª£c t√≠nh theo %.")
st.success("Ho√†n t·∫•t ph√¢n t√≠ch Clustering. M√¥ h√¨nh v√† k·∫øt qu·∫£ t√≥m t·∫Øt ƒë√£ s·∫µn s√†ng cho b√°o c√°o.")

# ========================================================
# L∆ØU MODEL (CHO C√ÅC B∆Ø·ªöC SAU)
# ========================================================
# *QUAN TR·ªåNG:* L∆∞u m√¥ h√¨nh K-Means v√† Preprocessor v√†o th∆∞ m·ª•c 'models/'
# ƒë·ªÉ c√°c b∆∞·ªõc Price Optimization (Elasticity) c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng m√† kh√¥ng c·∫ßn hu·∫•n luy·ªán l·∫°i.
# D√πng `pickle.dump` ƒë·ªÉ l∆∞u file .pkl (nh∆∞ ƒë√£ h∆∞·ªõng d·∫´n tr∆∞·ªõc ƒë√≥).
# B·∫°n c·∫ßn ch·∫°y ƒëo·∫°n n√†y *m·ªôt l·∫ßn* ·ªü local n·∫øu mu·ªën t√°i s·ª≠ d·ª•ng m√¥ h√¨nh trong c√°c file Streamlit kh√°c.
# st.write("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u cache. S·∫µn s√†ng cho Price Optimization.")